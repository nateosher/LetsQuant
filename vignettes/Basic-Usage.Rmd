---
title: "Basic Usage"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Basic-Usage}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(LetsQuant)
library(purrr)
library(tidyr)
library(dplyr)
library(magrittr)
library(ggplot2)
library(tibble)
```

# Summary

The goal of this vignette is to provide a basic example of how to use the `LetsQuant` package. We'll start by simulating a small data set:


```{r}
set.seed(11223344)
ex_data = map(1:30, \(i){
    n_obs = sample(100:150, 1)
    rgamma(n_obs, runif(1, 2, 10), runif(1, 2, 10)) %>%
      sort()
})
```

Note the data structure being used here is quite simple- it is just a list where each entry is a numeric vector sorted from smallest to largest. While in practice each entry in the list might represent a patient, image, etc., I'll refer to a numeric vector in this list as a "data entry." 

The first step is to form an overcomplete "dictionary" of beta quantile functions, and see which ones are useful for modeling our actual data. This will take the most time of the entire pipeline, since it entails running a cross-validated Lasso for each entry of our data list. Each data list can "select" any number of quantlets that explain the variance of that particular entry, though in practice each will only select a small handful. We will then aggregate these selections across all data entries for each quantlet to assess which explain the most variance across all data entries; higher selection counts indicate more importance, and lower selection counts indicate less importance. 

```{r}
selection_count_tib = GetSelectionCounts(ex_data,
                                   a = c(seq(0.1, 1, by = 0.2), seq(2, 100, by = 1)),
                                   b = c(seq(0.1, 1, by = 0.2), seq(2, 100, by = 1)),
                                   progress = FALSE)

glimpse(selection_count_tib)
```

The result is a tibble with four columns: `a`, `b`, `distribution`, and `selection_counts`. The first three store information about the quantile function used; the final column is the selection count as previously described. 

The goal of using a quantlet basis is to explain a lot of the variance using as small a number of quantlets as possible. So lets pick the top 7 to use as our basis. We can do this by passing the `selection_count_tib` to the `SelectQuantlets` function. The second argument determines the threshold at which a particular quantlet is included in the basis. If a quantlet is selected at least that many times, it is included in the basis.  

```{r}
quantlet_matrix = SelectQuantlets(selection_count_tib, 6)

class(quantlet_matrix)
dim(quantlet_matrix)
```

The result is a matrix where each column represents a selected quantlet, and each entry in that column represents the corresponding quantile function evaluated on a grid between 0 and 1. The size of the grid defaults to 1,024, so each column is $Q(\frac{1}{1,025}), \dots, Q(\frac{1,024}{1,025})$ for a given quantile function. Except there's a small catch, which you'll notice if you actually plot the resulting quantlet basis:

```{r}
PlotQuantletMatrix(quantlet_matrix)
```

These don't look like quantile functions. But this is just because `SelectQuantlets` automatically orthogonalizes the resulting basis, which is almost always desirable. But if you'd like to override this behavior, you can set the `orthogonalize` argument in `SelectQuantlets` to `FALSE`. 

```{r}
unorthogonalized_quantlet_matrix = SelectQuantlets(selection_count_tib, 6, 
                                                   orthogonalize = FALSE)

PlotQuantletMatrix(unorthogonalized_quantlet_matrix)
```

The resulting basis functions are still centered and scaled, but they look much more recognizably like quantile functions. 

Finally, to get the quantlet coefficients for each data set, we can use the `GetQuantletCoefficients` function:

```{r}
quantlet_coefs = GetQuantletCoefficients(ex_data, quantlet_matrix)
```

To see how the projected data compares to the actual data, we can get the projection by simply taking the product of the quantlet coefficients with the quantlet matrix. Here's how the first entry looks when projected:

```{r}
data_comparison = tibble(
  y = c(
    SameLengthData(ex_data, 1024)[[1]],
    as.numeric(quantlet_matrix %*% quantlet_coefs[[1]])
  ),
  p = rep((1:1024)/1025, 2),
  Data = c(rep("Actual", 1024), rep("Projected", 1024))
)

ggplot(data_comparison) + 
  geom_point(aes(x = p, y = y)) + 
  facet_wrap(~ Data) + 
  theme_bw()
```

This looks pretty good. However, you might be interested in a more rigorous assessment. After all, we chose the threshold of 6 somewhat arbitrarily- why not look at how different thresholds impact the "quality" of the projection?

That's the purpose of the `ScreePlot` function. If we pass it the real data and the selection count tibble, we can check the fidelity of the projections to the original data using whatever metric we're in. We specify a minimum selection threshold and a maximum threshold, as well as two summary metrics. `metric_1` determines how projections of a specific data are compared to the original data. A simple example would be to just look at the correlation between the projection and the original. 

However, note that we have 30 data entries, which means that for a single selection threshold, we'll get 30 summary statistics (and in general, $n$, where $n$ is the size of your data set). So we have to further summarize this somehow, e.g. take the mean of the correlations. We can then see how this summary changes as the selection threshold changes. Intuitively, as the selection threshold increases, we should expect see the fidelity decrease, even using a rough measure like Pearson correlation. This is, in fact, what we see:

```{r}
ScreePlot(ex_data, selection_count_tib,
          min_count = 6, max_count = 24,
          metric_1 = cor, metric_2 = min)
```

This can be a useful tool for setting the selection threshold. Here, we see that we might be able to get away with using even fewer quantlets- though seven is already a pretty small basis, so there isn't a lot of practical motivation to do this. However, this might depend somewhat on what our downstream analysis looks like.



